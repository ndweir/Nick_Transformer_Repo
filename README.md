# Transformer Hackathon üöÄ

Build your own GPT-style transformer model from scratch and compete on the leaderboard!

**üèÜ Leaderboard:** [https://huggingface.co/datasets/abhisu30/transformer-hackathon-leaderboard](https://huggingface.co/datasets/abhisu30/transformer-hackathon-leaderboard)

## Quick Start (3 Steps)

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Run the hackathon pipeline
python run_hackathon.py

# 3. Follow the prompts!
```

That's it! The script will automatically:
- Download TinyStories dataset (~50K stories)
- Train your model for 45 minutes
- Evaluate performance
- Upload to the leaderboard

---

## üìÅ Repository Structure

```
transformer-hackathon/
‚îú‚îÄ‚îÄ README.md                 # This file
‚îú‚îÄ‚îÄ COLAB_GUIDE.md           # Google Colab instructions
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ config.py                 # Hyperparameters (MODIFY THIS!)
‚îÇ
‚îú‚îÄ‚îÄ model/                    # üß† Transformer components
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py         # Token + positional embeddings
‚îÇ   ‚îú‚îÄ‚îÄ attention.py          # Multi-head self-attention
‚îÇ   ‚îú‚îÄ‚îÄ feedforward.py        # Feed-forward network
‚îÇ   ‚îú‚îÄ‚îÄ encoder_block.py      # Encoder layer
‚îÇ   ‚îú‚îÄ‚îÄ decoder_block.py      # Decoder layer
‚îÇ   ‚îú‚îÄ‚îÄ encoder.py            # Full encoder stack
‚îÇ   ‚îú‚îÄ‚îÄ decoder.py            # Full decoder stack
‚îÇ   ‚îî‚îÄ‚îÄ transformer.py        # Complete GPT model
‚îÇ
‚îú‚îÄ‚îÄ data/                     # üìö Data handling
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py          # Character/word tokenizer
‚îÇ   ‚îî‚îÄ‚îÄ dataset.py            # Dataset loading (TinyStories default)
‚îÇ
‚îú‚îÄ‚îÄ utils/                    # üîß Utilities
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py            # Evaluation metrics
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint.py         # Save/load models
‚îÇ   ‚îî‚îÄ‚îÄ huggingface_upload.py # Leaderboard integration
‚îÇ
‚îú‚îÄ‚îÄ train.py                  # Training script
‚îú‚îÄ‚îÄ evaluate.py               # Evaluation script
‚îú‚îÄ‚îÄ generate.py               # Text generation
‚îî‚îÄ‚îÄ run_hackathon.py          # üèÜ Main hackathon script
```

---

## üèóÔ∏è Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GPT-Style Transformer                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  Input: "The cat sat on the"                                    ‚îÇ
‚îÇ              ‚Üì                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  TOKEN EMBEDDING + POSITIONAL ENCODING                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Convert tokens to vectors                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Add position information                              ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ              ‚Üì                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ              DECODER BLOCK (√ó6)                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  MASKED MULTI-HEAD SELF-ATTENTION                  ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ Q, K, V projections                             ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ 8 attention heads                               ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ Causal masking (can only see past)              ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ              ‚Üì + Residual + LayerNorm                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  FEED-FORWARD NETWORK                              ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ Linear(512 ‚Üí 2048)                              ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ GELU activation                                 ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ Linear(2048 ‚Üí 512)                              ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ              ‚Üì + Residual + LayerNorm                    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ              ‚Üì                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  OUTPUT PROJECTION                                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Linear(512 ‚Üí vocab_size)                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Softmax ‚Üí probability distribution                    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ              ‚Üì                                                   ‚îÇ
‚îÇ  Output: "mat" (predicted next token)                           ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Default Configuration:
‚Ä¢ d_model = 512 (embedding dimension)
‚Ä¢ n_heads = 8 (attention heads)
‚Ä¢ n_layers = 6 (decoder blocks)
‚Ä¢ d_ff = 2048 (feed-forward dimension)
‚Ä¢ max_seq_len = 128 (context window)
‚Ä¢ vocab_size = ~65 (character-level)
‚Ä¢ Parameters = ~10M
```

---

## üèÜ Hackathon Rules

### Competition Categories

1. **ü•á Best Model Performance**
   - Lowest perplexity wins
   - Primary ranking metric

2. **‚ö° Most Efficient Training**
   - Highest tokens/second
   - Speed matters!

3. **üìù Best Generation Quality**
   - Highest Distinct-2 score
   - Text should be diverse and coherent

4. **üé® Most Creative Optimization**
   - Judged by organizers
   - Document your changes!

### Rules

- Training time: **Exactly 45 minutes**
- Hardware: Use whatever you have (GPU recommended)
- Code: Modify anything except timing enforcement
- Collaboration: Team up to 4 people

---

## üöÄ One-Click Colab Setup

Run this cell to set everything up instantly!

```python
# üöÄ ONE-CLICK SETUP
# Run this cell to set everything up!

# Check GPU
import torch
assert torch.cuda.is_available(), "‚ö†Ô∏è Enable GPU: Runtime > Change runtime type > GPU"

# Clone repository
!git clone https://github.com/abhishekadile/Transformer_Repo-.git
%cd Transformer_Repo-

# Install dependencies
!pip install -q torch numpy tqdm huggingface_hub datasets

# Start Hackathon!
# (Leaderboard upload happens automatically with embedded token)
!python run_hackathon.py
```

---

## üîß Optimization Ideas

Here are proven techniques to improve your model:

### Easy Wins üü¢

```python
# config.py
# 1. Enable mixed precision (2x speedup on modern GPUs!)
config.training.use_mixed_precision = True

# 2. Increase batch size if memory allows
config.training.batch_size = 32

# 3. Try different learning rates
config.training.learning_rate = 1e-3  # or 5e-4
```

### Medium Difficulty üü°

```python
# 1. Gradient accumulation for larger effective batch size
config.training.gradient_accumulation_steps = 4

# 2. Bigger model (if GPU memory allows)
config.model.d_model = 768
config.model.n_layers = 8

# 3. Better learning rate schedule
config.training.lr_scheduler = "cosine"
config.training.warmup_ratio = 0.1
```

### Advanced üî¥

1. **Flash Attention** (in `model/attention.py`)
   ```python
   # Replace manual attention with PyTorch's optimized version
   from torch.nn.functional import scaled_dot_product_attention
   ```

2. **SwiGLU Activation** (in `model/feedforward.py`)
   ```python
   # Use SwiGLUFeedForward instead of PositionwiseFeedForward
   from model.feedforward import SwiGLUFeedForward
   ```

3. **Gradient Checkpointing** (save memory for bigger models)
   ```python
   from torch.utils.checkpoint import checkpoint
   ```

4. **Custom Optimizer** (Lion, AdaFactor, etc.)

---

## üìä Understanding Your Metrics

| Metric | What It Measures | Good Value |
|--------|-----------------|------------|
| **Perplexity** | Model uncertainty (lower = better) | < 20 |
| **Loss** | Cross-entropy loss | < 3.0 |
| **Tokens/sec** | Training speed | > 2000 |
| **Distinct-2** | Generation diversity | > 0.5 |

---

## üêõ Troubleshooting

### Common Issues

**"CUDA out of memory"**
```python
# Reduce batch size
config.training.batch_size = 8

# Or enable gradient checkpointing
# (advanced, requires code changes)
```

**"Training is too slow"**
```python
# Enable mixed precision
config.training.use_mixed_precision = True

# Reduce model size
config.model.d_model = 256
config.model.n_layers = 4
```

**"Loss is not decreasing"**
```python
# Try lower learning rate
config.training.learning_rate = 1e-4

# Check for NaN (enable gradient clipping)
config.training.max_grad_norm = 0.5
```

**"Text generation is repetitive"**
```python
# Use higher temperature and repetition penalty
config.generation.temperature = 1.0
config.generation.repetition_penalty = 1.2
```

---

## üíª Running Individual Scripts

```bash
# Train only
python train.py --max-time 10  # 10 minute test run

# Train with custom settings
python train.py --batch-size 32 --lr 1e-3 --use-amp

# Evaluate a checkpoint
python evaluate.py --checkpoint checkpoints/best.pt --generate

# Generate text interactively
python generate.py

# Generate with custom settings
python generate.py --prompt "To be or not" --temperature 1.2 --max-tokens 200
```

---

## üìà Leaderboard

Results are uploaded to a shared Hugging Face dataset. View the leaderboard:

```python
from utils import display_leaderboard
display_leaderboard()
```

Or check online at: [Hugging Face Leaderboard](https://huggingface.co/datasets/transformer-hackathon/leaderboard)

---

## üéì Learning Resources

- [Attention Is All You Need (Original Paper)](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Andrej Karpathy's nanoGPT](https://github.com/karpathy/nanoGPT)
- [Hugging Face Transformers Course](https://huggingface.co/course)

---

## üìÑ License

MIT License - feel free to use, modify, and share!

---

**Good luck and have fun! üöÄ**
